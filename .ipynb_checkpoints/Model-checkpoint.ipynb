{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f02cc63e-3301-482c-8a30-7067c34b0337",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be07ad4c-1177-480f-9340-e19080a699d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\c640\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import heapq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Multiply, Dropout, Dense, BatchNormalization, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887d7426-8f53-48ff-8b9e-b3d007ef825f",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce93363-beff-4359-9b8b-745666e6362d",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "967ef78b-52c8-4ae5-b74b-07df4916e31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negatives(uids, iids, items, df_test):\n",
    "    \"\"\"\n",
    "    Returns a pandas dataframe of 50 negative interactions\n",
    "    based for each user in df_test.\n",
    "    \n",
    "    Args:\n",
    "        uids (np.array): Numpy array of all user ids.\n",
    "        iids (np.array): Numpy array of all item ids.\n",
    "        items (list): List of all unique items.\n",
    "        df_test (dataframe): Our test set.\n",
    "        \n",
    "    Returns:\n",
    "        df_neg (dataframe): dataframe with 100 negative items \n",
    "            for each (u, i) pair in df_test.\n",
    "    \"\"\"\n",
    "\n",
    "    negativeList = []\n",
    "    test_u = df_test['userID'].values.tolist()\n",
    "    test_i = df_test['vendorID'].values.tolist()\n",
    "\n",
    "    test_ratings = list(zip(test_u, test_i))\n",
    "    zipped = set(zip(uids, iids))\n",
    "\n",
    "    for (u, i) in test_ratings:\n",
    "        negatives = []\n",
    "        negatives.append((u, i))\n",
    "        for t in range(50):\n",
    "            j = np.random.randint(len(items)) # Get random item id.\n",
    "            while (u, j) in zipped: # Check if there is an interaction\n",
    "                j = np.random.randint(len(items)) # If yes, generate a new item id\n",
    "            negatives.append(j) # Once a negative interaction is found we add it.\n",
    "        negativeList.append(negatives)\n",
    "\n",
    "    df_neg = pd.DataFrame(negativeList)\n",
    "\n",
    "    return df_neg\n",
    "\n",
    "def mask_first(x):\n",
    "    \"\"\"\n",
    "    Return a list of 0 for the first item and 1 for all others\n",
    "    \"\"\"\n",
    "    result = np.ones_like(x)\n",
    "    result[0] = 0\n",
    "    \n",
    "    return result\n",
    "   \n",
    "def train_test_split(df):\n",
    "    \"\"\"\n",
    "    Splits our original data into one test and one\n",
    "    training set. \n",
    "    The test set is made up of one item for each user. This is\n",
    "    our holdout item used to compute Top@K later.\n",
    "    The training set is the same as our original data but\n",
    "    without any of the holdout items.\n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): Our original data\n",
    "        \n",
    "    Returns:\n",
    "        df_train (dataframe): All of our data except holdout items\n",
    "        df_test (dataframe): Only our holdout items.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create two copies of our dataframe that we can modify\n",
    "    df_test = df.copy(deep = True)\n",
    "    df_train = df.copy(deep = True)\n",
    "\n",
    "    # Group by user_id and select only the first item for\n",
    "    # each user (our holdout).\n",
    "    df_test = df_test.groupby(['userID']).first()\n",
    "    df_test['userID'] = df_test.index\n",
    "    df_test = df_test[['userID', 'vendorID', 'rating']]\n",
    "\n",
    "    # Remove the same items for our test set in our training set.\n",
    "    mask = df.groupby(['userID'])['userID'].transform(mask_first).astype(bool)\n",
    "    df_train = df.loc[mask]\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04e66f22-c93f-49b9-89fd-f1404f746966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Loads the dataset and transforms it into the format we need. \n",
    "    We then split it into a training and a test set.\n",
    "    \"\"\"\n",
    "\n",
    "    vendor = pd.read_csv('vendor.csv')\n",
    "    rating = pd.read_csv('rating.csv')\n",
    "    \n",
    "    vendor = vendor.head(500)\n",
    "\n",
    "    df = pd.merge(vendor, rating, on = 'vendorID', how = 'inner')\n",
    "    df = df[['userID', 'vendorID', 'rating']]\n",
    "\n",
    "    # Create training and test sets.\n",
    "    df_train, df_test = train_test_split(df)\n",
    "\n",
    "    # Create lists of all unique users and artists\n",
    "    users = list(np.sort(df['userID'].unique()))\n",
    "    items = list(np.sort(df['vendorID'].unique()))\n",
    "\n",
    "    # Get the rows, columns and values for our matrix.\n",
    "    rows = df_train['userID'].astype(int)\n",
    "    cols = df_train['vendorID'].astype(int)\n",
    "\n",
    "    values = list(df_train['rating'])\n",
    "\n",
    "    # Get all user ids and item ids.\n",
    "    uids = np.array(rows.tolist())\n",
    "    iids = np.array(cols.tolist())\n",
    "\n",
    "    # Sample negative interactions for each user in our test data\n",
    "    df_neg = get_negatives(uids, iids, items, df_test)\n",
    "\n",
    "    return uids, iids, df_train, df_test, df_neg, users, items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aec3ba-3f0e-4103-a20c-e28f25835662",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3273aac-dfaa-4c44-bf74-3af3157fd7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "uids, iids, df_train, df_test, df_neg, users, items = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06ffc62-2f92-49dc-aa5b-bcb9f969ba3e",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee17c43-9a2e-469d-8fc1-43112e4b3508",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "828f081e-634a-4a7a-9003-116e39173cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_list(U, I, L):\n",
    "    \"\"\"\n",
    "    Shuffle the data U, I, and L in the same order.\n",
    "    \n",
    "    Args:\n",
    "        U (list): All users for every interaction.\n",
    "        I (list): All items for every interaction.\n",
    "        L (list): All labels for every interaction.\n",
    "\n",
    "    Returns:\n",
    "        shuffled_U (list): Shuffled list of users.\n",
    "        shuffled_I (list): Shuffled list of items.\n",
    "        shuffled_L (list): Shuffled list of labels.\n",
    "    \"\"\"\n",
    "    combined = list(zip(U, I, L))\n",
    "    shuffle(combined)\n",
    "    shuffled_U, shuffled_I, shuffled_L = zip(*combined)\n",
    "\n",
    "    return shuffled_U, shuffled_I, shuffled_L\n",
    "\n",
    "\n",
    "def get_train_instances():\n",
    "     \"\"\"\n",
    "     Samples a number of negative user-item interactions for each\n",
    "     user-item pair in our testing data.\n",
    "     \n",
    "     Returns:\n",
    "         user_input (list): A list of all users for each item\n",
    "         item_input (list): A list of all items for every user,\n",
    "             both positive and negative interactions.\n",
    "         labels (list): A list of all labels. 0 or 1.\n",
    "     \"\"\"\n",
    "\n",
    "     user_input, item_input, labels = [], [], []\n",
    "     zipped = set(zip(uids, iids))\n",
    "\n",
    "     for (u, i) in zip(uids, iids):\n",
    "         # Add our positive interaction\n",
    "         user_input.append(u)\n",
    "         item_input.append(i)\n",
    "         labels.append(1)\n",
    "\n",
    "         # Sample a number of random negative interactions\n",
    "         for t in range(num_neg):\n",
    "             j = np.random.randint(len(items))\n",
    "             while (u, j) in zipped:\n",
    "                 j = np.random.randint(len(items))\n",
    "             user_input.append(u)\n",
    "             item_input.append(j)\n",
    "             labels.append(0)\n",
    "\n",
    "     return user_input, item_input, labels\n",
    "\n",
    "\n",
    "def random_mini_batches(U, I, L, steps = 20):\n",
    "    \"\"\"\n",
    "    Returns a list of shuffeled mini batched of a given size.\n",
    "    \n",
    "    Args:\n",
    "        U (list): All users for every interaction \n",
    "        I (list): All items for every interaction\n",
    "        L (list): All labels for every interaction.\n",
    "    \n",
    "    Returns:\n",
    "        mini_batches (list): A list of minibatches containing sets\n",
    "            of batch users, batch items and batch labels \n",
    "            [(u, i, l), (u, i, l) ...]\n",
    "    \"\"\"\n",
    "\n",
    "    mini_batches = []\n",
    "\n",
    "    shuffled_U, shuffled_I, shuffled_L = shuffle_list(U, I, L)\n",
    "\n",
    "    mini_batch_size = int(math.ceil(len(U) / steps))\n",
    "    for k in range(steps):\n",
    "        start_idx = k * mini_batch_size\n",
    "        end_idx = min((k + 1) * mini_batch_size, len(U))\n",
    "\n",
    "        mini_batch_U = shuffled_U[start_idx:end_idx]\n",
    "        mini_batch_I = shuffled_I[start_idx:end_idx]\n",
    "        mini_batch_L = shuffled_L[start_idx:end_idx]\n",
    "\n",
    "        mini_batch = (mini_batch_U, mini_batch_I, mini_batch_L)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "def get_hits(k_ranked, holdout):\n",
    "    \"\"\"\n",
    "    Return 1 if an item exists in a given list and 0 if not.\n",
    "    \"\"\"\n",
    "\n",
    "    for item in k_ranked:\n",
    "        if item == holdout:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def predict_ratings_cf(user_idx, items, model):\n",
    "    \"\"\"\n",
    "    Predict rating score for each user.\n",
    "\n",
    "    Args:\n",
    "        user_idx (int): Current index\n",
    "        test_ratings (list): Our test set user-item pairs\n",
    "        test_negatives (list): 100 negative items for each\n",
    "            user in our test set.\n",
    "        K (int): number of top recommendations\n",
    "        \n",
    "    Returns:\n",
    "        map_item_score (list): predicted current user rating for each item.\n",
    "    \"\"\"\n",
    "    # Prepare user and item arrays for the model.\n",
    "    predict_user = np.full(len(items), user_idx, dtype = 'int32').reshape(-1, 1)\n",
    "    np_items = np.array(items).reshape(-1, 1)\n",
    "\n",
    "    # Predict ratings using the model.\n",
    "    predictions = model.predict([predict_user, np_items]).flatten().tolist()\n",
    "\n",
    "    # Map predicted score to item id.\n",
    "    map_item_score = dict(zip(items, predictions))\n",
    "\n",
    "    return map_item_score\n",
    "    \n",
    "\n",
    "def eval_rating(idx, test_ratings, test_negatives, K, model):\n",
    "    \"\"\"\n",
    "    Generate ratings for the users in our test set and\n",
    "    check if our holdout item is among the top K highest scores.\n",
    "    \n",
    "    Args:\n",
    "        idx (int): Current index\n",
    "        test_ratings (list): Our test set user-item pairs\n",
    "        test_negatives (list): negative items for each\n",
    "            user in our test set.\n",
    "        K (int): number of top recommendations\n",
    "        \n",
    "    Returns:\n",
    "        hitrate (list): A list of 1 if the holdout appeared in our\n",
    "            top K predicted items. 0 if not.\n",
    "    \"\"\"\n",
    "    # Get the negative interactions for our user.\n",
    "    items = test_negatives[idx]\n",
    "\n",
    "    # Get the user idx.\n",
    "    user_idx = test_ratings[idx][0]\n",
    "\n",
    "    # Get the item idx, i.e., our holdout item.\n",
    "    holdout = test_ratings[idx][1]\n",
    "\n",
    "    # Add the holdout to the end of the negative interactions list.\n",
    "    items.append(holdout)\n",
    "\n",
    "    # Predict ratings using the model.\n",
    "    map_item_score = predict_ratings_cf(user_idx, items, model)\n",
    "\n",
    "    # Get the K highest ranked items as a list.\n",
    "    k_ranked = heapq.nlargest(K, map_item_score, key = map_item_score.get)\n",
    "\n",
    "    # Get a list of hit or no hit.\n",
    "    hitrate = get_hits(k_ranked, holdout)\n",
    "\n",
    "    return hitrate\n",
    "\n",
    "\n",
    "def evaluate(model, df_neg, K = 10):\n",
    "    \"\"\"\n",
    "    Calculate the top@K hit ratio for our recommendations.\n",
    "    \n",
    "    Args:\n",
    "        df_neg (dataframe): dataframe containing our holdout items\n",
    "            and 100 randomly sampled negative interactions for each\n",
    "            (user, item) holdout pair.\n",
    "        K (int): The 'K' number of ranked predictions we want\n",
    "            our holdout item to be present in. \n",
    "            \n",
    "    Returns:\n",
    "        hits (list): list of \"hits\". 1 if the holdout was present in \n",
    "            the K highest ranked predictions. 0 if not. \n",
    "    \"\"\"\n",
    "\n",
    "    hits = []\n",
    "\n",
    "    test_u = df_test['userID'].values.tolist()\n",
    "    test_i = df_test['vendorID'].values.tolist()\n",
    "\n",
    "    test_ratings = list(zip(test_u, test_i))\n",
    "\n",
    "    df_neg = df_neg.drop(df_neg.columns[0], axis = 1)\n",
    "    test_negatives = df_neg.values.tolist()\n",
    "\n",
    "    for idx in range(len(test_ratings)):\n",
    "        # For each idx, call eval_one_rating\n",
    "        hitrate = eval_rating(idx, test_ratings, test_negatives, K, model)\n",
    "        hits.append(hitrate)\n",
    "\n",
    "    return hits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a723669-9d35-47ee-8da9-425c9058f1f9",
   "metadata": {},
   "source": [
    "## Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00e19e03-9876-4ab7-ab68-b8cf616ce528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMS\n",
    "num_neg = 10\n",
    "latent_features = 20\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e14a1d3-a88f-45f3-bbba-cef94c59c06c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\c640\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " user (InputLayer)           [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " item (InputLayer)           [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " mlp_user_embedding (Embedd  (None, 1, 64)                32064     ['user[0][0]']                \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " mlp_item_embedding (Embedd  (None, 1, 64)                32064     ['item[0][0]']                \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)         (None, 64)                   0         ['mlp_user_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)         (None, 64)                   0         ['mlp_item_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 128)                  0         ['flatten_2[0][0]',           \n",
      "                                                                     'flatten_3[0][0]']           \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 128)                  0         ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " mlp_layer1 (Dense)          (None, 128)                  16512     ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " mlp_batch_norm1 (BatchNorm  (None, 128)                  512       ['mlp_layer1[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " mlp_dropout1 (Dropout)      (None, 128)                  0         ['mlp_batch_norm1[0][0]']     \n",
      "                                                                                                  \n",
      " mlp_layer2 (Dense)          (None, 64)                   8256      ['mlp_dropout1[0][0]']        \n",
      "                                                                                                  \n",
      " mlp_batch_norm2 (BatchNorm  (None, 64)                   256       ['mlp_layer2[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " gmf_user_embedding (Embedd  (None, 1, 20)                10020     ['user[0][0]']                \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " gmf_item_embedding (Embedd  (None, 1, 20)                10020     ['item[0][0]']                \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " mlp_dropout2 (Dropout)      (None, 64)                   0         ['mlp_batch_norm2[0][0]']     \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 20)                   0         ['gmf_user_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 20)                   0         ['gmf_item_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " mlp_layer3 (Dense)          (None, 32)                   2080      ['mlp_dropout2[0][0]']        \n",
      "                                                                                                  \n",
      " multiply (Multiply)         (None, 20)                   0         ['flatten[0][0]',             \n",
      "                                                                     'flatten_1[0][0]']           \n",
      "                                                                                                  \n",
      " mlp_layer4 (Dense)          (None, 16)                   528       ['mlp_layer3[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 36)                   0         ['multiply[0][0]',            \n",
      " )                                                                   'mlp_layer4[0][0]']          \n",
      "                                                                                                  \n",
      " output_layer (Dense)        (None, 1)                    37        ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 112349 (438.86 KB)\n",
      "Trainable params: 111965 (437.36 KB)\n",
      "Non-trainable params: 384 (1.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# TENSORFLOW GRAPH\n",
    "# Using the functional API\n",
    "\n",
    "# Define input layers for user, item, and label.\n",
    "user_input = Input(shape = (1,), dtype = tf.int32, name = 'user')\n",
    "item_input = Input(shape = (1,), dtype = tf.int32, name = 'item')\n",
    "label_input = Input(shape = (1,), dtype = tf.int32, name = 'label')\n",
    "\n",
    "# User embedding for MLP\n",
    "mlp_user_embedding = Embedding(input_dim = len(users) + 1, \n",
    "                               output_dim = 64,\n",
    "                               embeddings_initializer = 'random_normal',\n",
    "                               embeddings_regularizer = None,\n",
    "                               input_length = 1, \n",
    "                               name = 'mlp_user_embedding')(user_input)\n",
    "\n",
    "# Item embedding for MLP\n",
    "mlp_item_embedding = Embedding(input_dim = len(items) + 1, \n",
    "                               output_dim = 64,\n",
    "                               embeddings_initializer = 'random_normal',\n",
    "                               embeddings_regularizer = None,\n",
    "                               input_length = 1, \n",
    "                               name = 'mlp_item_embedding')(item_input)\n",
    "\n",
    "# User embedding for GMF\n",
    "gmf_user_embedding = Embedding(input_dim = len(users) + 1, \n",
    "                               output_dim = latent_features,\n",
    "                               embeddings_initializer = 'random_normal',\n",
    "                               embeddings_regularizer = None,\n",
    "                               input_length = 1, \n",
    "                               name = 'gmf_user_embedding')(user_input)\n",
    "\n",
    "# Item embedding for GMF\n",
    "gmf_item_embedding = Embedding(input_dim = len(items) + 1, \n",
    "                               output_dim = latent_features,\n",
    "                               embeddings_initializer = 'random_normal',\n",
    "                               embeddings_regularizer = None,\n",
    "                               input_length = 1, \n",
    "                               name = 'gmf_item_embedding')(item_input)\n",
    "\n",
    "# GMF layers\n",
    "gmf_user_flat = Flatten()(gmf_user_embedding)\n",
    "gmf_item_flat = Flatten()(gmf_item_embedding)\n",
    "gmf_matrix = Multiply()([gmf_user_flat, gmf_item_flat])\n",
    "\n",
    "# MLP layers\n",
    "mlp_user_flat = Flatten()(mlp_user_embedding)\n",
    "mlp_item_flat = Flatten()(mlp_item_embedding)\n",
    "mlp_concat = Concatenate()([mlp_user_flat, mlp_item_flat])\n",
    "\n",
    "mlp_dropout = Dropout(0.1)(mlp_concat)\n",
    "\n",
    "mlp_layer_1 = Dense(128, \n",
    "                    activation = 'relu', \n",
    "                    name = 'mlp_layer1')(mlp_dropout)\n",
    "mlp_batch_norm1 = BatchNormalization(name = 'mlp_batch_norm1')(mlp_layer_1)\n",
    "mlp_dropout1 = Dropout(0.1, \n",
    "                       name = 'mlp_dropout1')(mlp_batch_norm1)\n",
    "\n",
    "mlp_layer_2 = Dense(64, \n",
    "                    activation = 'relu', \n",
    "                    name = 'mlp_layer2')(mlp_dropout1)\n",
    "mlp_batch_norm2 = BatchNormalization(name = 'mlp_batch_norm2')(mlp_layer_2)\n",
    "mlp_dropout2 = Dropout(0.1, \n",
    "                       name = 'mlp_dropout2')(mlp_batch_norm2)\n",
    "\n",
    "mlp_layer_3 = Dense(32, \n",
    "                    activation = 'relu', \n",
    "                    name = 'mlp_layer3')(mlp_dropout2)\n",
    "mlp_layer_4 = Dense(16, \n",
    "                    activation = 'relu', \n",
    "                    name = 'mlp_layer4')(mlp_layer_3)\n",
    "\n",
    "# Merge the two networks\n",
    "merged_vector = Concatenate()([gmf_matrix, mlp_layer_4])\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(1, \n",
    "                     activation = 'sigmoid',\n",
    "                     kernel_initializer = 'lecun_uniform',\n",
    "                     name = 'output_layer')(merged_vector)\n",
    "\n",
    "# Define the model\n",
    "modelCF = Model(inputs = [user_input, item_input], outputs = output_layer)\n",
    "\n",
    "# Compile the model with binary cross entropy loss and Adam optimizer\n",
    "optimizer = Adam(learning_rate = learning_rate)\n",
    "modelCF.compile(optimizer = optimizer,\n",
    "                loss = 'binary_crossentropy',\n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "modelCF.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeb6ece0-33e3-46e1-bd78-467c6801231e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\c640\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\src\\backend.py:5818: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Loss: 0.38052868843078613\n",
      "Epoch: 2 - Loss: 0.32030877470970154\n",
      "Epoch: 3 - Loss: 0.310343861579895\n",
      "Epoch: 4 - Loss: 0.3000347912311554\n",
      "Epoch: 5 - Loss: 0.3044221103191376\n",
      "Epoch: 6 - Loss: 0.2875482141971588\n",
      "Epoch: 7 - Loss: 0.30285927653312683\n",
      "Epoch: 8 - Loss: 0.29651686549186707\n",
      "Epoch: 9 - Loss: 0.2945338487625122\n",
      "Epoch: 10 - Loss: 0.2936088740825653\n",
      "Epoch: 11 - Loss: 0.2937958240509033\n",
      "Epoch: 12 - Loss: 0.28404706716537476\n",
      "Epoch: 13 - Loss: 0.2932332456111908\n",
      "Epoch: 14 - Loss: 0.29272016882896423\n",
      "Epoch: 15 - Loss: 0.2852868437767029\n",
      "Epoch: 16 - Loss: 0.2895868122577667\n",
      "Epoch: 17 - Loss: 0.27733784914016724\n",
      "Epoch: 18 - Loss: 0.2683286964893341\n",
      "Epoch: 19 - Loss: 0.2758727967739105\n",
      "Epoch: 20 - Loss: 0.2694667875766754\n",
      "Epoch: 21 - Loss: 0.24723386764526367\n",
      "Epoch: 22 - Loss: 0.2549269497394562\n",
      "Epoch: 23 - Loss: 0.26222091913223267\n",
      "Epoch: 24 - Loss: 0.2546091079711914\n",
      "Epoch: 25 - Loss: 0.2546660304069519\n",
      "Epoch: 26 - Loss: 0.2418176680803299\n",
      "Epoch: 27 - Loss: 0.23952315747737885\n",
      "Epoch: 28 - Loss: 0.24011802673339844\n",
      "Epoch: 29 - Loss: 0.23491829633712769\n",
      "Epoch: 30 - Loss: 0.23066465556621552\n",
      "Epoch: 31 - Loss: 0.23725281655788422\n",
      "Epoch: 32 - Loss: 0.227888822555542\n",
      "Epoch: 33 - Loss: 0.2200939655303955\n",
      "Epoch: 34 - Loss: 0.2133646309375763\n",
      "Epoch: 35 - Loss: 0.2155657857656479\n",
      "Epoch: 36 - Loss: 0.2129097431898117\n",
      "Epoch: 37 - Loss: 0.1949838399887085\n",
      "Epoch: 38 - Loss: 0.2062525749206543\n",
      "Epoch: 39 - Loss: 0.1990882158279419\n",
      "Epoch: 40 - Loss: 0.20387941598892212\n",
      "Epoch: 41 - Loss: 0.19764764606952667\n",
      "Epoch: 42 - Loss: 0.19695039093494415\n",
      "Epoch: 43 - Loss: 0.18885840475559235\n",
      "Epoch: 44 - Loss: 0.19442829489707947\n",
      "Epoch: 45 - Loss: 0.18926794826984406\n",
      "Epoch: 46 - Loss: 0.1889299750328064\n",
      "Epoch: 47 - Loss: 0.18544965982437134\n",
      "Epoch: 48 - Loss: 0.17249296605587006\n",
      "Epoch: 49 - Loss: 0.1776120364665985\n",
      "Epoch: 50 - Loss: 0.17136865854263306\n",
      "Epoch: 51 - Loss: 0.16778473556041718\n",
      "Epoch: 52 - Loss: 0.1763792634010315\n",
      "Epoch: 53 - Loss: 0.1668708473443985\n",
      "Epoch: 54 - Loss: 0.16641905903816223\n",
      "Epoch: 55 - Loss: 0.16852492094039917\n",
      "Epoch: 56 - Loss: 0.17035682499408722\n",
      "Epoch: 57 - Loss: 0.1679840385913849\n",
      "Epoch: 58 - Loss: 0.16609540581703186\n",
      "Epoch: 59 - Loss: 0.1563190519809723\n",
      "Epoch: 60 - Loss: 0.161622554063797\n",
      "Epoch: 61 - Loss: 0.16441285610198975\n",
      "Epoch: 62 - Loss: 0.15709632635116577\n",
      "Epoch: 63 - Loss: 0.15626837313175201\n",
      "Epoch: 64 - Loss: 0.15954570472240448\n",
      "Epoch: 65 - Loss: 0.15505363047122955\n",
      "Epoch: 66 - Loss: 0.15808406472206116\n",
      "Epoch: 67 - Loss: 0.1555526703596115\n",
      "Epoch: 68 - Loss: 0.15033890306949615\n",
      "Epoch: 69 - Loss: 0.15617908537387848\n",
      "Epoch: 70 - Loss: 0.14670918881893158\n",
      "Epoch: 71 - Loss: 0.14357922971248627\n",
      "Epoch: 72 - Loss: 0.15285462141036987\n",
      "Epoch: 73 - Loss: 0.1548893004655838\n",
      "Epoch: 74 - Loss: 0.14863698184490204\n",
      "Epoch: 75 - Loss: 0.15052835643291473\n",
      "Epoch: 76 - Loss: 0.13920265436172485\n",
      "Epoch: 77 - Loss: 0.13703760504722595\n",
      "Epoch: 78 - Loss: 0.13765224814414978\n",
      "Epoch: 79 - Loss: 0.1420820951461792\n",
      "Epoch: 80 - Loss: 0.1375797539949417\n",
      "Epoch: 81 - Loss: 0.13754603266716003\n",
      "Epoch: 82 - Loss: 0.13298577070236206\n",
      "Epoch: 83 - Loss: 0.14061535894870758\n",
      "Epoch: 84 - Loss: 0.14904187619686127\n",
      "Epoch: 85 - Loss: 0.13385619223117828\n",
      "Epoch: 86 - Loss: 0.13433103263378143\n",
      "Epoch: 87 - Loss: 0.13235722482204437\n",
      "Epoch: 88 - Loss: 0.13039936125278473\n",
      "Epoch: 89 - Loss: 0.12797029316425323\n",
      "Epoch: 90 - Loss: 0.1360500156879425\n",
      "Epoch: 91 - Loss: 0.1314697414636612\n",
      "Epoch: 92 - Loss: 0.13843640685081482\n",
      "Epoch: 93 - Loss: 0.13274462521076202\n",
      "Epoch: 94 - Loss: 0.13170064985752106\n",
      "Epoch: 95 - Loss: 0.1256408840417862\n",
      "Epoch: 96 - Loss: 0.12735740840435028\n",
      "Epoch: 97 - Loss: 0.13337837159633636\n",
      "Epoch: 98 - Loss: 0.14129148423671722\n",
      "Epoch: 99 - Loss: 0.13083197176456451\n",
      "Epoch: 100 - Loss: 0.1269061416387558\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Get our training input.\n",
    "    user_input, item_input, labels = get_train_instances()\n",
    "\n",
    "    # Generate a list of minibatches.\n",
    "    minibatches = random_mini_batches(user_input, item_input, labels)\n",
    "\n",
    "    # Loop over each batch and fit the model\n",
    "    for iter, minibatch in enumerate(minibatches):\n",
    "        batch_user_input = np.array(minibatch[0]).reshape(-1, 1)\n",
    "        batch_item_input = np.array(minibatch[1]).reshape(-1, 1)\n",
    "        batch_labels = np.array(minibatch[2]).reshape(-1, 1)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            logits = modelCF([batch_user_input, batch_item_input], training = True)\n",
    "            loss_value = tf.keras.losses.BinaryCrossentropy(from_logits = True)(batch_labels, logits)\n",
    "            loss_value = tf.reduce_mean(loss_value)\n",
    "\n",
    "        # Backward pass\n",
    "        grads = tape.gradient(loss_value, modelCF.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, modelCF.trainable_variables))\n",
    "\n",
    "    # Print the loss after each epoch\n",
    "    print(f'Epoch: {epoch + 1} - Loss: {loss_value.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "335d2b4c-f003-4da4-b084-ec29f0810aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 529us/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 548us/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 504us/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 510us/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 539us/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 509us/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "0.134\n"
     ]
    }
   ],
   "source": [
    "# Calculate top@K    \n",
    "hits = evaluate(modelCF, df_neg)\n",
    "print(np.array(hits).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7740dc9d-30db-43fb-a151-dabe3e15f074",
   "metadata": {},
   "source": [
    "## API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2554437-f683-4de3-9269-36a5e122659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_CF():\n",
    "    # HYPERPARAMS\n",
    "    num_neg = 10\n",
    "    latent_features = 20\n",
    "    epochs = 100\n",
    "    batch_size = 256\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # TENSORFLOW GRAPH\n",
    "    # Using the functional API\n",
    "    \n",
    "    # Define input layers for user, item, and label.\n",
    "    user_input = Input(shape = (1,), dtype = tf.int32, name = 'user')\n",
    "    item_input = Input(shape = (1,), dtype = tf.int32, name = 'item')\n",
    "    label_input = Input(shape = (1,), dtype = tf.int32, name = 'label')\n",
    "    \n",
    "    # User embedding for MLP\n",
    "    mlp_user_embedding = Embedding(input_dim = len(users) + 1, \n",
    "                                   output_dim = 64,\n",
    "                                   embeddings_initializer = 'random_normal',\n",
    "                                   embeddings_regularizer = None,\n",
    "                                   input_length = 1, \n",
    "                                   name = 'mlp_user_embedding')(user_input)\n",
    "    \n",
    "    # Item embedding for MLP\n",
    "    mlp_item_embedding = Embedding(input_dim = len(items) + 1, \n",
    "                                   output_dim = 64,\n",
    "                                   embeddings_initializer = 'random_normal',\n",
    "                                   embeddings_regularizer = None,\n",
    "                                   input_length = 1, \n",
    "                                   name = 'mlp_item_embedding')(item_input)\n",
    "    \n",
    "    # User embedding for GMF\n",
    "    gmf_user_embedding = Embedding(input_dim = len(users) + 1, \n",
    "                                   output_dim = latent_features,\n",
    "                                   embeddings_initializer = 'random_normal',\n",
    "                                   embeddings_regularizer = None,\n",
    "                                   input_length = 1, \n",
    "                                   name = 'gmf_user_embedding')(user_input)\n",
    "    \n",
    "    # Item embedding for GMF\n",
    "    gmf_item_embedding = Embedding(input_dim = len(items) + 1, \n",
    "                                   output_dim = latent_features,\n",
    "                                   embeddings_initializer = 'random_normal',\n",
    "                                   embeddings_regularizer = None,\n",
    "                                   input_length = 1, \n",
    "                                   name = 'gmf_item_embedding')(item_input)\n",
    "    \n",
    "    # GMF layers\n",
    "    gmf_user_flat = Flatten()(gmf_user_embedding)\n",
    "    gmf_item_flat = Flatten()(gmf_item_embedding)\n",
    "    gmf_matrix = Multiply()([gmf_user_flat, gmf_item_flat])\n",
    "    \n",
    "    # MLP layers\n",
    "    mlp_user_flat = Flatten()(mlp_user_embedding)\n",
    "    mlp_item_flat = Flatten()(mlp_item_embedding)\n",
    "    mlp_concat = Concatenate()([mlp_user_flat, mlp_item_flat])\n",
    "    \n",
    "    mlp_dropout = Dropout(0.1)(mlp_concat)\n",
    "    \n",
    "    mlp_layer_1 = Dense(128, \n",
    "                        activation = 'relu', \n",
    "                        name = 'mlp_layer1')(mlp_dropout)\n",
    "    mlp_batch_norm1 = BatchNormalization(name = 'mlp_batch_norm1')(mlp_layer_1)\n",
    "    mlp_dropout1 = Dropout(0.1, \n",
    "                           name = 'mlp_dropout1')(mlp_batch_norm1)\n",
    "    \n",
    "    mlp_layer_2 = Dense(64, \n",
    "                        activation = 'relu', \n",
    "                        name = 'mlp_layer2')(mlp_dropout1)\n",
    "    mlp_batch_norm2 = BatchNormalization(name = 'mlp_batch_norm2')(mlp_layer_2)\n",
    "    mlp_dropout2 = Dropout(0.1, \n",
    "                           name = 'mlp_dropout2')(mlp_batch_norm2)\n",
    "    \n",
    "    mlp_layer_3 = Dense(32, \n",
    "                        activation = 'relu', \n",
    "                        name = 'mlp_layer3')(mlp_dropout2)\n",
    "    mlp_layer_4 = Dense(16, \n",
    "                        activation = 'relu', \n",
    "                        name = 'mlp_layer4')(mlp_layer_3)\n",
    "    \n",
    "    # Merge the two networks\n",
    "    merged_vector = Concatenate()([gmf_matrix, mlp_layer_4])\n",
    "    \n",
    "    # Output layer\n",
    "    output_layer = Dense(1, \n",
    "                         activation = 'sigmoid',\n",
    "                         kernel_initializer = 'lecun_uniform',\n",
    "                         name = 'output_layer')(merged_vector)\n",
    "    \n",
    "    # Define the model\n",
    "    modelCF = Model(inputs = [user_input, item_input], outputs = output_layer)\n",
    "    \n",
    "    # Compile the model with binary cross entropy loss and Adam optimizer\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    "    modelCF.compile(optimizer = optimizer,\n",
    "                    loss = 'binary_crossentropy',\n",
    "                    metrics = ['accuracy'])\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Get our training input.\n",
    "        user_input, item_input, labels = get_train_instances()\n",
    "    \n",
    "        # Generate a list of minibatches.\n",
    "        minibatches = random_mini_batches(user_input, item_input, labels)\n",
    "    \n",
    "        # Loop over each batch and fit the model\n",
    "        for iter, minibatch in enumerate(minibatches):\n",
    "            batch_user_input = np.array(minibatch[0]).reshape(-1, 1)\n",
    "            batch_item_input = np.array(minibatch[1]).reshape(-1, 1)\n",
    "            batch_labels = np.array(minibatch[2]).reshape(-1, 1)\n",
    "    \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Forward pass\n",
    "                logits = modelCF([batch_user_input, batch_item_input], training = True)\n",
    "                loss_value = tf.keras.losses.BinaryCrossentropy(from_logits = True)(batch_labels, logits)\n",
    "                loss_value = tf.reduce_mean(loss_value)\n",
    "    \n",
    "            # Backward pass\n",
    "            grads = tape.gradient(loss_value, modelCF.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, modelCF.trainable_variables))\n",
    "    \n",
    "    modelCF.save('recsys/model/modelCF.h5', hist)\n",
    "    print(\"Model has been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a675ab04-d817-4584-b077-590e1a551060",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_CF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efb9f352-c603-487e-ac55-9af1acb9873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_items(dct, k = 10):\n",
    "    # Use nlargest to get the top n key-value pairs based on values.\n",
    "    top_k_items = heapq.nlargest(k, dct.items(), key = lambda item: item[1])\n",
    "\n",
    "    return top_k_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e00a78-7444-4f3d-bd19-a2539343ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "origins = [\n",
    "    \"http://localhost\",\n",
    "    \"http://localhost:3000\",\n",
    "]\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins = origins,\n",
    "    allow_credentials = True,\n",
    "    allow_methods = [\"*\"],\n",
    "    allow_headers = [\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"Hello World\"}\n",
    "\n",
    "\n",
    "class Recommendation(BaseModel):\n",
    "    userID: str\n",
    "\n",
    "\n",
    "@app.post('/recsys/train')\n",
    "def train():\n",
    "    train_model_CF()\n",
    "    '''\n",
    "    train_model_CB()\n",
    "    '''\n",
    "\n",
    "    return 'Model trained successfully.'\n",
    "\n",
    "@app.post('/recsys/recommend')\n",
    "async def recommendItem(req: Recommendation):\n",
    "    userID = req.userID\n",
    "\n",
    "    # Load CF model\n",
    "    modelCF = load_model('recsys/model/modelCF.h5')\n",
    "    \n",
    "    try:\n",
    "        # Predict ratings using CF model\n",
    "        cf_works = True\n",
    "        ratingsCF = predict_ratings_cf(\n",
    "            user_idx = userID,\n",
    "            pass\n",
    "        )\n",
    "        \n",
    "        recommendations = get_top_k_items(ratings, k = 10)\n",
    "        average_ratings = np.mean([value for key, value in recommendations.items()])\n",
    "    \n",
    "        # If predicted rating avg. is less than 4, then CF model fails\n",
    "        if average_ratings >= 4:\n",
    "            recommendation_items = [key for key, value in recommendations.items()]\n",
    "    \n",
    "        # Else proceed to predict using CB model\n",
    "        cf_works = False\n",
    "        \n",
    "        '''\n",
    "        FILL THE CODE HERE FOR CB\n",
    "        '''\n",
    "\n",
    "        dct = {\n",
    "            'status': 200,\n",
    "            'message': \"recommendation for user has been successfully get.\",\n",
    "            'data': {\n",
    "                'user_id': userID,\n",
    "                'recommendations': recommendation_items\n",
    "                },\n",
    "            'success': True\n",
    "            'error': None\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        recommendation_items = []\n",
    "        dct = {\n",
    "            'status': 404,\n",
    "            'message': \"recommendation for user has failed.\",\n",
    "            'data': {\n",
    "                'user_id': userID,\n",
    "                'recommendations': recommendation_items\n",
    "                },\n",
    "            'success': False,\n",
    "            'error': e\n",
    "        }\n",
    "\n",
    "    return dct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
